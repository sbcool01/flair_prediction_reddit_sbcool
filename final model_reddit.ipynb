{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import math\nimport json\nimport requests\nimport itertools\nimport numpy as np\nimport time\nimport datetime","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"'2.1.0'"},"metadata":{}}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\n\n## Plot\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\nimport matplotlib as plt\n\n# NLTK\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\n# Other\nimport re\nimport string\nimport numpy as np\nimport pandas as pd\nfrom sklearn.manifold import TSNE","execution_count":4,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(r'/kaggle/input/raw-4-years-data-for-corpus/reddit_data_raw_4years.csv')","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=[]\n\nfor title in df['title']:\n    corpus.append(title)\n    \nlen(corpus)","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"433802"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"wpt = nltk.WordPunctTokenizer()\nstop_words = nltk.corpus.stopwords.words('english')\n\ndef normalize_document(doc):\n    try:\n        # lower case and remove special characters\\whitespaces\n        doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n        doc = doc.lower()\n        doc = doc.strip()\n        # tokenize document\n        tokens = wpt.tokenize(doc)\n        # filter stopwords out of document\n        filtered_tokens = [token for token in tokens if token not in stop_words]\n        # re-create document from filtered tokens\n        doc = ' '.join(filtered_tokens)\n        return doc\n    except:\n        return \"\"\n\nnormalize_corpus = np.vectorize(normalize_document)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_corpus = normalize_corpus(corpus)\nnorm_corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_NB_WORDS = 30000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 30\n# This is fixed.\nEMBEDDING_DIM = 300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import word2vec\n\n# tokenize sentences in corpus\nwpt = nltk.WordPunctTokenizer()\ntokenized_corpus = [wpt.tokenize(document) for document in norm_corpus]\n\n# Set values for various parameters\nfeature_size = EMBEDDING_DIM    # Word vector dimensionality  \nwindow_context = 6          # Context window size                                                                                    \nmin_word_count = 1   # Minimum word count                        \nsample = 1e-3   # Downsample setting for frequent words\n\nw2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, \n                          window=window_context, min_count=min_word_count,\n                          sample=sample, iter=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = w2v_model.wv.index2word\nwvs = w2v_model.wv[words]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=pd.read_csv('/kaggle/input/reddit-data-balanced/reddit_data_balanced.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=df_train.loc[df_train['flair']!='other']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus2=[]\n\nfor title in df_train['title']:\n    corpus2.append(title)\n    \nlen(corpus2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_corpus2 = normalize_corpus(corpus2)\nnorm_corpus2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(norm_corpus2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_tokenized_train = tokenizer.texts_to_sequences(norm_corpus2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pad_sequences(list_tokenized_train, maxlen=MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Found %s unique tokens.' % len(tokenizer.word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_dict={}\n\nfor word in w2v_model.wv.vocab:\n    t_dict[word]=w2v_model[word]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# t_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_embs = np.stack(t_dict.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nemb_mean,emb_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\nnb_words = min(MAX_NB_WORDS, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i >= MAX_NB_WORDS: continue\n    embedding_vector = t_dict.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['category_id'] = df_train['flair'].factorize()[0]\ncategory_id_df = df_train[['flair', 'category_id']].drop_duplicates().sort_values('category_id')\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['category_id', 'flair']].values)\ndf_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels=np.reshape(np.array(df_train['category_id']), (83000, 1))\nfrom keras.utils import to_categorical\ny_binary = to_categorical(labels)\n\n# labels2=np.reshape(np.array(df_train2['category_id']), (118000, 1))\n# from keras.utils import to_categorical\n# y_binary2 = to_categorical(labels2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_binary.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n\n# model = Sequential()\n# # x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n# model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix]))\n# # model.add(SpatialDropout1D(0.2))\n# model.add(LSTM(512, dropout=0.2, recurrent_dropout=0.2))\n# model.add(Dense(14, activation='softmax'))\n# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# print(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Dense, Activation\nfrom keras import optimizers\n\nmodel = Sequential()\n# x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(1000, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, kernel_initializer='glorot_uniform'))\n# model.add(GlobalAveragePooling1D())\n# model.add(BatchNormalization())\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(13, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n# from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n# from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n# from keras.layers.normalization import BatchNormalization\n# from keras.layers import Dense, Activation\n\n# model = Sequential()\n# # x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n# model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix]))\n# model.add(Conv1D(filters=64, kernel_size=3 ,strides=1, padding='same' , activation= 'relu')) \n# model.add(MaxPooling1D(pool_size=2))\n# model.add(SpatialDropout1D(0.4))\n# model.add(LSTM(1000, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, kernel_initializer='glorot_uniform'))\n# # model.add(GlobalAveragePooling1D())\n# # model.add(BatchNormalization())\n# model.add(GlobalMaxPooling1D())\n# model.add(Dense(14, activation='softmax'))\n# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# print(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n# from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n# from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n# from keras.layers.normalization import BatchNormalization\n# from keras.layers import Dense, Activation\n# from keras.models import Model\n\n\n# inp = Input( shape=(30,))\n# x=Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix])(inp)\n# x=SpatialDropout1D(0.2)(x)\n# x=LSTM(1000, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(x)\n# x=BatchNormalization()(x)\n# x=Activation('relu')(x)\n# avg_pool = GlobalAveragePooling1D()(x)\n# max_pool = GlobalMaxPooling1D()(x)\n# conc = concatenate([avg_pool, max_pool])\n# out=Dense(14, activation='softmax')(conc)\n\n# model = Model(inputs=inp, outputs=out)\n# model.compile(loss='logcosh', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n\n# model = Sequential()\n# # x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n# model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix]))\n# model.add(SpatialDropout1D(0.2))\n# model.add(LSTM(1000, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n# model.add(GlobalMaxPooling1D())\n# model.add(Dense(14, activation='softmax'))\n# model.compile(loss=tf.keras.losses.Huber(delta=1.0), optimizer='adam', metrics=['accuracy'])\n# print(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 10\nbatch_size = 64\n\nhistory = model.fit(X[0:70000], y_binary[0:70000],\n                    batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=model.predict(X[70000:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_report(df_train['category_id'][70000:], np.argmax(y_pred, axis=1), output_dict=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('/kaggle/working/reddit_predictor.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_json = model.to_json()\nwith open(\"/kaggle/working/model.json\", \"w\") as json_file:\n    json_file.write(model_json)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}