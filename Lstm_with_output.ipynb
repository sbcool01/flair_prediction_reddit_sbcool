{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "## Plot\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "import matplotlib as plt\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Other\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'/kaggle/input/raw-4-years-data-for-corpus/reddit_data_raw_4years.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433802"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=[]\n",
    "\n",
    "for title in df['title']:\n",
    "    corpus.append(title)\n",
    "    \n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    try:\n",
    "        # lower case and remove special characters\\whitespaces\n",
    "        doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "        doc = doc.lower()\n",
    "        doc = doc.strip()\n",
    "        # tokenize document\n",
    "        tokens = wpt.tokenize(doc)\n",
    "        # filter stopwords out of document\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        # re-create document from filtered tokens\n",
    "        doc = ' '.join(filtered_tokens)\n",
    "        return doc\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cash cow venezuelas oil company verges collapse',\n",
       "       'new study squelches treasured theory indians originsthe aryans come india conquered',\n",
       "       'indias garment exports may hit billion fy', ..., 'aayega toh',\n",
       "       'junior hockey world cup india claim title win belgium',\n",
       "       'ribs broken money gone stranded german grateful support gurdwaras np'],\n",
       "      dtype='<U300')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 30000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# tokenize sentences in corpus\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in norm_corpus]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = EMBEDDING_DIM    # Word vector dimensionality  \n",
    "window_context = 6          # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, \n",
    "                          window=window_context, min_count=min_word_count,\n",
    "                          sample=sample, iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = w2v_model.wv.index2word\n",
    "wvs = w2v_model.wv[words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('/kaggle/input/reddit-data-balanced/reddit_data_balanced.csv')\n",
    "# df_train2=pd.read_csv('/kaggle/input/reddit-balanced-modified/reddit_data_balanced_modified_27-4-2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HELP HELP TEST</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lets have a conversation Randians</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Forest guards ordered to watch over python tha...</td>\n",
       "      <td>Non-Political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Engineering pass-outs from Shitty colleges (Ti...</td>\n",
       "      <td>AskIndia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Constitution, as ABVP would have it. [Old]</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Pune city</td>\n",
       "      <td>Photography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Chicken Tikka Masala - Chicken Tikka Gravy</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Rangoli Chandel loses calm after Twitter warni...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>fake followers data of media</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>'Death of currency' not a new subject, virtual...</td>\n",
       "      <td>Demonetization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Fire Mock Drill At Pesh Infotech Hinjawadi, ph...</td>\n",
       "      <td>Business/Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>I have been reading posts on other India relat...</td>\n",
       "      <td>AskIndia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Redditors Share Inside Stories Of How People W...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>India vs New Zealand: Hardik Pandya ruled out ...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>সিআইডির জেরার মুখে কৈলাস -রুপা,তদন্তে সব সাহায...</td>\n",
       "      <td>Not in English.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Demonetization: In hindsight</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Read all the 100's of replies to this tweet, r...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Isro earns 6.1 million euros from 29 nanosatel...</td>\n",
       "      <td>Science/Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Encounter between terrorists, security forces ...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Indians, I got a question. Do you drink cows u...</td>\n",
       "      <td>AskIndia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>When is our independence day India? (see billb...</td>\n",
       "      <td>AskIndia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Amazon India Plans to Scrap Single-Use Plastic...</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>How to enroll in Google Free Augmented and Vir...</td>\n",
       "      <td>Science/Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>hey r/india what are some good sources to buy ...</td>\n",
       "      <td>AskIndia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Indian superstar Aamir Khan tackles taboos one...</td>\n",
       "      <td>Non-Political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>Great India Festive Sale Organic natural honey...</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>Sleeping beauties (Sparrows), [Ambikapur, Chha...</td>\n",
       "      <td>Photography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>Kumar Gandharva. Nirguani Bhajan - Guruji Jaha...</td>\n",
       "      <td>Non-Political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>Luka Modric Pips Cristiano Ronaldo, Mo Salah T...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>Don’t come home for Eid, a Kashmiri mother tel...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                              title  \\\n",
       "0            0                                     HELP HELP TEST   \n",
       "1            1                  Lets have a conversation Randians   \n",
       "2            2  Forest guards ordered to watch over python tha...   \n",
       "3            3  Engineering pass-outs from Shitty colleges (Ti...   \n",
       "4            4     The Constitution, as ABVP would have it. [Old]   \n",
       "5            5                                          Pune city   \n",
       "6            6         Chicken Tikka Masala - Chicken Tikka Gravy   \n",
       "7            7  Rangoli Chandel loses calm after Twitter warni...   \n",
       "8            8                       fake followers data of media   \n",
       "9            9  'Death of currency' not a new subject, virtual...   \n",
       "10          10  Fire Mock Drill At Pesh Infotech Hinjawadi, ph...   \n",
       "11          11  I have been reading posts on other India relat...   \n",
       "12          12  Redditors Share Inside Stories Of How People W...   \n",
       "13          13  India vs New Zealand: Hardik Pandya ruled out ...   \n",
       "14          14  সিআইডির জেরার মুখে কৈলাস -রুপা,তদন্তে সব সাহায...   \n",
       "15          15                       Demonetization: In hindsight   \n",
       "16          16  Read all the 100's of replies to this tweet, r...   \n",
       "17          17  Isro earns 6.1 million euros from 29 nanosatel...   \n",
       "18          18  Encounter between terrorists, security forces ...   \n",
       "19          19  Indians, I got a question. Do you drink cows u...   \n",
       "20          20  When is our independence day India? (see billb...   \n",
       "21          21  Amazon India Plans to Scrap Single-Use Plastic...   \n",
       "22          22  How to enroll in Google Free Augmented and Vir...   \n",
       "23          23  hey r/india what are some good sources to buy ...   \n",
       "24          24  Indian superstar Aamir Khan tackles taboos one...   \n",
       "25          25  Great India Festive Sale Organic natural honey...   \n",
       "26          26  Sleeping beauties (Sparrows), [Ambikapur, Chha...   \n",
       "27          27  Kumar Gandharva. Nirguani Bhajan - Guruji Jaha...   \n",
       "28          28  Luka Modric Pips Cristiano Ronaldo, Mo Salah T...   \n",
       "29          29  Don’t come home for Eid, a Kashmiri mother tel...   \n",
       "\n",
       "                 flair  \n",
       "0        [R]eddiquette  \n",
       "1        [R]eddiquette  \n",
       "2        Non-Political  \n",
       "3             AskIndia  \n",
       "4             Politics  \n",
       "5          Photography  \n",
       "6                 Food  \n",
       "7             Politics  \n",
       "8        [R]eddiquette  \n",
       "9       Demonetization  \n",
       "10    Business/Finance  \n",
       "11            AskIndia  \n",
       "12               other  \n",
       "13              Sports  \n",
       "14     Not in English.  \n",
       "15               other  \n",
       "16               other  \n",
       "17  Science/Technology  \n",
       "18            Politics  \n",
       "19            AskIndia  \n",
       "20            AskIndia  \n",
       "21       [R]eddiquette  \n",
       "22  Science/Technology  \n",
       "23            AskIndia  \n",
       "24       Non-Political  \n",
       "25                Food  \n",
       "26         Photography  \n",
       "27       Non-Political  \n",
       "28              Sports  \n",
       "29            Politics  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.loc[df_train['flair']!='other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83000, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2=[]\n",
    "\n",
    "for title in df_train['title']:\n",
    "    corpus2.append(title)\n",
    "    \n",
    "len(corpus2)\n",
    "\n",
    "# corpus3=[]\n",
    "\n",
    "# for title in df_train2['title']:\n",
    "#     corpus3.append(title)\n",
    "    \n",
    "# len(corpus3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['help help test', 'lets conversation randians',\n",
       "       'forest guards ordered watch python swallowed deer', ...,\n",
       "       'whatsapp getting ready worlds bigvest election',\n",
       "       'india japan plan obor alternative project unveiled next monday',\n",
       "       'np mumbai feel everything shifting gujarat cm vijay rupani indian express'],\n",
       "      dtype='<U286')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus2 = normalize_corpus(corpus2)\n",
    "norm_corpus2\n",
    "\n",
    "# norm_corpus3 = normalize_corpus(corpus3)\n",
    "# norm_corpus3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(norm_corpus2)\n",
    "\n",
    "# tokenizer2 = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "# tokenizer2.fit_on_texts(norm_corpus3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokenized_train = tokenizer.texts_to_sequences(norm_corpus2)\n",
    "\n",
    "# list_tokenized_train2 = tokenizer2.texts_to_sequences(norm_corpus3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(list_tokenized_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# X2 = pad_sequences(list_tokenized_train2, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51226 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s unique tokens.' % len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_dict={}\n",
    "\n",
    "for word in w2v_model.wv.vocab:\n",
    "    t_dict[word]=w2v_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3254: FutureWarning:\n",
      "\n",
      "arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.00039438746, 0.366756)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(t_dict.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS: continue\n",
    "    embedding_vector = t_dict.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# word_index2 = tokenizer2.word_index\n",
    "# nb_words2 = min(MAX_NB_WORDS, len(word_index2))\n",
    "# embedding_matrix2 = np.random.normal(emb_mean, emb_std, (nb_words2, EMBEDDING_DIM))\n",
    "# for word, i in word_index2.items():\n",
    "#     if i >= MAX_NB_WORDS: continue\n",
    "#     embedding_vector = t_dict.get(word)\n",
    "#     if embedding_vector is not None: embedding_matrix2[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.54035758e-01, -7.14715725e-01, -3.77627676e-01, -3.42692560e-01,\n",
       "        3.60031046e-02, -1.22589549e-02, -3.23111645e-01,  6.18959673e-02,\n",
       "       -1.22807597e+00,  8.61130617e-01, -4.25768456e-01, -7.83921861e-02,\n",
       "       -2.89682581e-01, -3.90151753e-01, -5.50786936e-01, -6.45206111e-01,\n",
       "       -2.69191365e-01, -2.46128844e-01,  6.28626516e-01,  4.39771890e-01,\n",
       "        2.07312066e-01,  3.85357659e-01,  3.98939635e-01,  6.13101030e-02,\n",
       "        1.14850893e-01, -3.69680888e-01, -2.47556722e-02, -3.73322505e-01,\n",
       "        3.23721058e-01,  2.63995182e-01, -1.37180337e-01,  1.32020673e-01,\n",
       "        1.48231016e-01,  6.59202017e-02,  3.76199717e-02, -2.50323107e-02,\n",
       "        2.45526469e-01,  4.00565316e-02, -8.86870992e-01,  1.50214185e-01,\n",
       "       -3.34767641e-01, -4.64592434e-01,  2.37097084e-01, -4.43036339e-01,\n",
       "       -1.77844703e-01,  7.42442460e-02,  8.36147984e-02,  5.17094480e-01,\n",
       "       -1.06315766e-01, -2.81727410e-01, -4.56499929e-03,  7.68575157e-01,\n",
       "       -3.21768668e-02,  1.38920817e-01, -1.87041172e-01,  1.47900718e-01,\n",
       "        1.89160024e-01, -1.14326214e-01,  3.88235631e-01,  5.31574960e-01,\n",
       "        1.48264834e-01,  1.83036315e-01,  1.60302728e-01, -2.34542519e-01,\n",
       "       -3.99042003e-01, -8.00634936e-01, -1.18878430e-01, -5.95782806e-01,\n",
       "        3.75520369e-01, -3.45193627e-01, -3.65292741e-01, -3.53507686e-01,\n",
       "       -2.00529712e-01, -3.57856930e-01, -1.50827041e-01, -3.69003623e-01,\n",
       "        1.13100684e-02,  3.28907804e-01,  1.60023826e-01, -1.77376497e-01,\n",
       "       -1.59061980e-01, -7.87597360e-01, -1.74438012e-01,  1.98961281e-01,\n",
       "       -4.40974420e-01,  5.54752254e-01, -1.48506059e-01, -3.30790555e-01,\n",
       "       -4.56092355e-01,  3.69675194e-01, -1.43690389e-01, -8.76096250e-01,\n",
       "       -5.67316168e-01,  7.80027924e-01, -2.04121028e-01, -1.57243143e-01,\n",
       "       -9.23393789e-01, -2.00049869e-01,  7.53410429e-01,  1.10405489e-01,\n",
       "        2.84987676e-01,  1.83747597e-01, -1.38299712e-01,  4.89208916e-01,\n",
       "       -2.70329319e-01, -1.58046784e-01,  4.85465569e-01,  4.10821621e-02,\n",
       "       -3.97800501e-01,  1.61513028e-01,  7.39352581e-03,  5.26293431e-01,\n",
       "        2.55414018e-01, -3.43721570e-02, -3.42848204e-01,  2.32841904e-01,\n",
       "        4.46454410e-01, -1.87597882e-01, -1.27126617e-01,  6.12120035e-02,\n",
       "        2.31717303e-01, -4.22640807e-01, -1.00638929e-02, -1.47984494e-01,\n",
       "       -2.12930219e-01,  1.57094653e-01, -2.59405749e-01,  5.80596433e-01,\n",
       "        4.37070635e-01, -3.89332658e-01,  7.56135005e-01, -8.39043395e-02,\n",
       "       -3.63767982e-01,  6.15934093e-02, -8.29620520e-01, -2.50827137e-01,\n",
       "       -8.59325072e-02,  9.28592437e-01, -1.95852886e-01, -6.03015788e-01,\n",
       "       -3.51003540e-01, -9.54468445e-02,  2.36506036e-01,  4.12252534e-01,\n",
       "       -3.47650591e-01,  8.87521579e-02, -3.34461324e-01, -4.74824618e-02,\n",
       "       -3.46651349e-01, -6.36731185e-03, -2.43751600e-02,  1.63251867e-01,\n",
       "       -1.53746508e-03,  5.46138803e-02,  1.15260849e-01, -7.27078785e-02,\n",
       "       -1.87136593e-01, -2.01300440e-04,  9.13127164e-02,  1.56654695e-01,\n",
       "       -1.92163478e-01,  1.69796457e-01, -5.87270451e-01, -4.98364451e-01,\n",
       "       -1.85568056e-01, -1.39233442e-01, -4.71945849e-01, -7.07964413e-01,\n",
       "        3.81058976e-01,  3.52269661e-01,  2.62068234e-01,  2.64645991e-01,\n",
       "        3.73304692e-01, -1.12320115e-01,  5.22087818e-01,  2.40406984e-01,\n",
       "       -8.53001251e-02, -8.97852900e-01, -1.99774297e-01, -2.71115851e-01,\n",
       "        2.74645580e-01,  6.40712029e-01,  5.36753231e-02, -1.52226605e-01,\n",
       "       -1.11643491e-01, -3.08437846e-01,  1.49956798e-01,  3.76385032e-01,\n",
       "       -1.60177423e-01, -2.77789754e-01, -5.36252615e-01,  6.61473049e-03,\n",
       "        1.18175143e-01, -3.21795124e-01, -7.45028233e-01, -2.97713649e-02,\n",
       "        1.47404423e-01,  4.84424968e-01, -4.38976237e-01,  2.81740614e-01,\n",
       "        6.62428780e-01,  2.88177256e-01, -7.00050176e-01, -9.20387629e-01,\n",
       "       -5.98714203e-02, -4.22536433e-02,  6.34891678e-01,  1.46302092e-01,\n",
       "        5.75293980e-01, -3.39019753e-01, -2.82901580e-02, -3.06969660e-01,\n",
       "       -4.41553193e-02,  6.47047428e-01,  6.88459179e-03,  2.69220205e-01,\n",
       "       -1.47560346e-01, -5.13417812e-01,  1.17367537e-01, -1.40432522e-01,\n",
       "       -1.74908463e-01, -2.56979298e-01,  8.32838484e-02, -3.02128811e-01,\n",
       "        6.90712891e-01,  3.17859544e-02,  3.16615981e-01, -9.21355325e-02,\n",
       "       -8.42204470e-01, -7.73913131e-01,  7.30895858e-01, -3.99585129e-02,\n",
       "       -1.33685365e-01, -3.76908349e-01, -1.40904029e-01,  3.86601345e-01,\n",
       "        4.47955813e-01,  7.18750093e-02,  5.38492752e-01,  1.94139942e-01,\n",
       "        9.53812729e-01,  6.96746640e-02, -2.55851111e-01,  1.79041950e-01,\n",
       "        4.21136398e-01,  1.31989417e-01,  1.29003593e-01, -4.83285324e-01,\n",
       "        1.37998189e-01,  2.36366143e-02, -4.25562582e-01,  3.52408834e-01,\n",
       "        8.92748943e-02,  5.61837620e-01,  9.06051798e-02, -1.47578759e-01,\n",
       "       -3.64041678e-01,  1.33317131e-01, -2.00509695e-01,  1.57713057e-01,\n",
       "       -1.53999640e-01,  5.02505796e-01, -1.38053654e-01, -1.96808494e-01,\n",
       "       -9.66529271e-02, -2.94617127e-01, -1.70783580e-01, -8.37883305e-02,\n",
       "       -2.43041697e-01,  2.37480458e-01,  1.59551604e-01, -2.43858708e-01,\n",
       "       -3.12521305e-02,  2.07212563e-01,  2.55749589e-01,  1.02943995e-01,\n",
       "       -3.30688430e-01,  4.29487813e-01, -2.71498380e-02, -6.10773618e-01,\n",
       "        4.84189720e-02, -4.48107666e-01,  1.86729370e-01, -1.83454296e-01,\n",
       "       -5.01576847e-01, -3.40120236e-01, -3.36162815e-01,  4.99940198e-01,\n",
       "        7.64796126e-01,  2.00840289e-01,  2.06882474e-01,  4.64401320e-01,\n",
       "        6.20677281e-02,  1.36498643e-01,  2.09010316e-01, -8.35621722e-02,\n",
       "        2.85476137e-01,  3.55433513e-01,  2.25590687e-01,  2.04592567e-01])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HELP HELP TEST</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lets have a conversation Randians</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Forest guards ordered to watch over python tha...</td>\n",
       "      <td>Non-Political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Engineering pass-outs from Shitty colleges (Ti...</td>\n",
       "      <td>AskIndia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Constitution, as ABVP would have it. [Old]</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0                                     HELP HELP TEST   \n",
       "1           1                  Lets have a conversation Randians   \n",
       "2           2  Forest guards ordered to watch over python tha...   \n",
       "3           3  Engineering pass-outs from Shitty colleges (Ti...   \n",
       "4           4     The Constitution, as ABVP would have it. [Old]   \n",
       "\n",
       "           flair  \n",
       "0  [R]eddiquette  \n",
       "1  [R]eddiquette  \n",
       "2  Non-Political  \n",
       "3       AskIndia  \n",
       "4       Politics  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>flair</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HELP HELP TEST</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lets have a conversation Randians</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Forest guards ordered to watch over python tha...</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Engineering pass-outs from Shitty colleges (Ti...</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Constitution, as ABVP would have it. [Old]</td>\n",
       "      <td>Politics</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0                                     HELP HELP TEST   \n",
       "1           1                  Lets have a conversation Randians   \n",
       "2           2  Forest guards ordered to watch over python tha...   \n",
       "3           3  Engineering pass-outs from Shitty colleges (Ti...   \n",
       "4           4     The Constitution, as ABVP would have it. [Old]   \n",
       "\n",
       "           flair  category_id  \n",
       "0  [R]eddiquette            0  \n",
       "1  [R]eddiquette            0  \n",
       "2  Non-Political            1  \n",
       "3       AskIndia            2  \n",
       "4       Politics            3  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['category_id'] = df_train['flair'].factorize()[0]\n",
    "category_id_df = df_train[['flair', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'flair']].values)\n",
    "df_train.head()\n",
    "\n",
    "# df_train2['category_id'] = df_train2['flair'].factorize()[0]\n",
    "# category_id_df2 = df_train2[['flair', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "# category_to_id2 = dict(category_id_df2.values)\n",
    "# id_to_category2 = dict(category_id_df2[['category_id', 'flair']].values)\n",
    "# df_train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>flair</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HELP HELP TEST</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lets have a conversation Randians</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Forest guards ordered to watch over python tha...</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Engineering pass-outs from Shitty colleges (Ti...</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Constitution, as ABVP would have it. [Old]</td>\n",
       "      <td>Politics</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Pune city</td>\n",
       "      <td>Photography</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Chicken Tikka Masala - Chicken Tikka Gravy</td>\n",
       "      <td>Food</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Rangoli Chandel loses calm after Twitter warni...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>fake followers data of media</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>'Death of currency' not a new subject, virtual...</td>\n",
       "      <td>Demonetization</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Fire Mock Drill At Pesh Infotech Hinjawadi, ph...</td>\n",
       "      <td>Business/Finance</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>I have been reading posts on other India relat...</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>India vs New Zealand: Hardik Pandya ruled out ...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>সিআইডির জেরার মুখে কৈলাস -রুপা,তদন্তে সব সাহায...</td>\n",
       "      <td>Not in English.</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Isro earns 6.1 million euros from 29 nanosatel...</td>\n",
       "      <td>Science/Technology</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Encounter between terrorists, security forces ...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Indians, I got a question. Do you drink cows u...</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>When is our independence day India? (see billb...</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Amazon India Plans to Scrap Single-Use Plastic...</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>How to enroll in Google Free Augmented and Vir...</td>\n",
       "      <td>Science/Technology</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>hey r/india what are some good sources to buy ...</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Indian superstar Aamir Khan tackles taboos one...</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>Great India Festive Sale Organic natural honey...</td>\n",
       "      <td>Food</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>Sleeping beauties (Sparrows), [Ambikapur, Chha...</td>\n",
       "      <td>Photography</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>Kumar Gandharva. Nirguani Bhajan - Guruji Jaha...</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>Luka Modric Pips Cristiano Ronaldo, Mo Salah T...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>Don’t come home for Eid, a Kashmiri mother tel...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>30 dogs at Mohali shelter ‘attacked with acid,...</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>Shell cos clampdown: Chandy, Sasikala among 10...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>Kolkata’s new fine dining restaurant is all ab...</td>\n",
       "      <td>Food</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                              title  \\\n",
       "0            0                                     HELP HELP TEST   \n",
       "1            1                  Lets have a conversation Randians   \n",
       "2            2  Forest guards ordered to watch over python tha...   \n",
       "3            3  Engineering pass-outs from Shitty colleges (Ti...   \n",
       "4            4     The Constitution, as ABVP would have it. [Old]   \n",
       "5            5                                          Pune city   \n",
       "6            6         Chicken Tikka Masala - Chicken Tikka Gravy   \n",
       "7            7  Rangoli Chandel loses calm after Twitter warni...   \n",
       "8            8                       fake followers data of media   \n",
       "9            9  'Death of currency' not a new subject, virtual...   \n",
       "10          10  Fire Mock Drill At Pesh Infotech Hinjawadi, ph...   \n",
       "11          11  I have been reading posts on other India relat...   \n",
       "13          13  India vs New Zealand: Hardik Pandya ruled out ...   \n",
       "14          14  সিআইডির জেরার মুখে কৈলাস -রুপা,তদন্তে সব সাহায...   \n",
       "17          17  Isro earns 6.1 million euros from 29 nanosatel...   \n",
       "18          18  Encounter between terrorists, security forces ...   \n",
       "19          19  Indians, I got a question. Do you drink cows u...   \n",
       "20          20  When is our independence day India? (see billb...   \n",
       "21          21  Amazon India Plans to Scrap Single-Use Plastic...   \n",
       "22          22  How to enroll in Google Free Augmented and Vir...   \n",
       "23          23  hey r/india what are some good sources to buy ...   \n",
       "24          24  Indian superstar Aamir Khan tackles taboos one...   \n",
       "25          25  Great India Festive Sale Organic natural honey...   \n",
       "26          26  Sleeping beauties (Sparrows), [Ambikapur, Chha...   \n",
       "27          27  Kumar Gandharva. Nirguani Bhajan - Guruji Jaha...   \n",
       "28          28  Luka Modric Pips Cristiano Ronaldo, Mo Salah T...   \n",
       "29          29  Don’t come home for Eid, a Kashmiri mother tel...   \n",
       "30          30  30 dogs at Mohali shelter ‘attacked with acid,...   \n",
       "31          31  Shell cos clampdown: Chandy, Sasikala among 10...   \n",
       "32          32  Kolkata’s new fine dining restaurant is all ab...   \n",
       "\n",
       "                 flair  category_id  \n",
       "0        [R]eddiquette            0  \n",
       "1        [R]eddiquette            0  \n",
       "2        Non-Political            1  \n",
       "3             AskIndia            2  \n",
       "4             Politics            3  \n",
       "5          Photography            4  \n",
       "6                 Food            5  \n",
       "7             Politics            3  \n",
       "8        [R]eddiquette            0  \n",
       "9       Demonetization            6  \n",
       "10    Business/Finance            7  \n",
       "11            AskIndia            2  \n",
       "13              Sports            8  \n",
       "14     Not in English.            9  \n",
       "17  Science/Technology           10  \n",
       "18            Politics            3  \n",
       "19            AskIndia            2  \n",
       "20            AskIndia            2  \n",
       "21       [R]eddiquette            0  \n",
       "22  Science/Technology           10  \n",
       "23            AskIndia            2  \n",
       "24       Non-Political            1  \n",
       "25                Food            5  \n",
       "26         Photography            4  \n",
       "27       Non-Political            1  \n",
       "28              Sports            8  \n",
       "29            Politics            3  \n",
       "30       Non-Political            1  \n",
       "31            Politics            3  \n",
       "32                Food            5  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=np.reshape(np.array(df_train['category_id']), (83000, 1))\n",
    "from keras.utils import to_categorical\n",
    "y_binary = to_categorical(labels)\n",
    "\n",
    "# labels2=np.reshape(np.array(df_train2['category_id']), (118000, 1))\n",
    "# from keras.utils import to_categorical\n",
    "# y_binary2 = to_categorical(labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 300)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83000, 13)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "\n",
    "# model = Sequential()\n",
    "# # x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "# model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix]))\n",
    "# # model.add(SpatialDropout1D(0.2))\n",
    "# model.add(LSTM(512, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(14, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 300)         9000000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, None, 300)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 1000)        5204000   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 13)                13013     \n",
      "=================================================================\n",
      "Total params: 14,217,013\n",
      "Trainable params: 14,217,013\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "# x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(1000, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, kernel_initializer='glorot_uniform'))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "# model.add(BatchNormalization())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "# from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "# from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# from keras.layers import Dense, Activation\n",
    "\n",
    "# model = Sequential()\n",
    "# # x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "# model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix]))\n",
    "# model.add(Conv1D(filters=64, kernel_size=3 ,strides=1, padding='same' , activation= 'relu')) \n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(SpatialDropout1D(0.4))\n",
    "# model.add(LSTM(1000, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, kernel_initializer='glorot_uniform'))\n",
    "# # model.add(GlobalAveragePooling1D())\n",
    "# # model.add(BatchNormalization())\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(14, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "# from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "# from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# from keras.layers import Dense, Activation\n",
    "# from keras.models import Model\n",
    "\n",
    "\n",
    "# inp = Input( shape=(30,))\n",
    "# x=Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix])(inp)\n",
    "# x=SpatialDropout1D(0.2)(x)\n",
    "# x=LSTM(1000, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(x)\n",
    "# x=BatchNormalization()(x)\n",
    "# x=Activation('relu')(x)\n",
    "# avg_pool = GlobalAveragePooling1D()(x)\n",
    "# max_pool = GlobalMaxPooling1D()(x)\n",
    "# conc = concatenate([avg_pool, max_pool])\n",
    "# out=Dense(14, activation='softmax')(conc)\n",
    "\n",
    "# model = Model(inputs=inp, outputs=out)\n",
    "# model.compile(loss='logcosh', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "\n",
    "# model = Sequential()\n",
    "# # x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "# model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights=[embedding_matrix]))\n",
    "# model.add(SpatialDropout1D(0.2))\n",
    "# model.add(LSTM(1000, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(14, activation='softmax'))\n",
    "# model.compile(loss=tf.keras.losses.Huber(delta=1.0), optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83000, 30)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning:\n",
      "\n",
      "Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 56000 samples, validate on 14000 samples\n",
      "Epoch 1/10\n",
      "56000/56000 [==============================] - 45s 803us/step - loss: 1.5675 - accuracy: 0.4922 - val_loss: 1.4078 - val_accuracy: 0.5510\n",
      "Epoch 2/10\n",
      "56000/56000 [==============================] - 44s 789us/step - loss: 1.3965 - accuracy: 0.5484 - val_loss: 1.3843 - val_accuracy: 0.5595\n",
      "Epoch 3/10\n",
      "56000/56000 [==============================] - 44s 778us/step - loss: 1.3171 - accuracy: 0.5719 - val_loss: 1.3704 - val_accuracy: 0.5692\n",
      "Epoch 4/10\n",
      "56000/56000 [==============================] - 44s 794us/step - loss: 1.2532 - accuracy: 0.5914 - val_loss: 1.3642 - val_accuracy: 0.5727\n",
      "Epoch 5/10\n",
      "56000/56000 [==============================] - 43s 769us/step - loss: 1.1855 - accuracy: 0.6121 - val_loss: 1.3794 - val_accuracy: 0.5730\n",
      "Epoch 6/10\n",
      "56000/56000 [==============================] - 44s 786us/step - loss: 1.1219 - accuracy: 0.6326 - val_loss: 1.3977 - val_accuracy: 0.5720\n",
      "Epoch 7/10\n",
      "56000/56000 [==============================] - 44s 782us/step - loss: 1.0709 - accuracy: 0.6493 - val_loss: 1.4130 - val_accuracy: 0.5754\n",
      "Epoch 8/10\n",
      "56000/56000 [==============================] - 43s 777us/step - loss: 1.0140 - accuracy: 0.6662 - val_loss: 1.4408 - val_accuracy: 0.5694\n",
      "Epoch 9/10\n",
      "56000/56000 [==============================] - 43s 775us/step - loss: 0.9699 - accuracy: 0.6799 - val_loss: 1.4637 - val_accuracy: 0.5693\n",
      "Epoch 10/10\n",
      "56000/56000 [==============================] - 44s 782us/step - loss: 0.9217 - accuracy: 0.6971 - val_loss: 1.4927 - val_accuracy: 0.5746\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X[0:70000], y_binary[0:70000],\n",
    "                    batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>flair</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HELP HELP TEST</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lets have a conversation Randians</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Forest guards ordered to watch over python tha...</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Engineering pass-outs from Shitty colleges (Ti...</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Constitution, as ABVP would have it. [Old]</td>\n",
       "      <td>Politics</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0                                     HELP HELP TEST   \n",
       "1           1                  Lets have a conversation Randians   \n",
       "2           2  Forest guards ordered to watch over python tha...   \n",
       "3           3  Engineering pass-outs from Shitty colleges (Ti...   \n",
       "4           4     The Constitution, as ABVP would have it. [Old]   \n",
       "\n",
       "           flair  category_id  \n",
       "0  [R]eddiquette            0  \n",
       "1  [R]eddiquette            0  \n",
       "2  Non-Political            1  \n",
       "3       AskIndia            2  \n",
       "4       Politics            3  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X[70000:])\n",
    "# y_pred2=model.predict(X2[90000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['flair'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.4636363636363636,\n",
       "  'recall': 0.1951530612244898,\n",
       "  'f1-score': 0.2746858168761221,\n",
       "  'support': 784},\n",
       " '1': {'precision': 0.3847184986595174,\n",
       "  'recall': 0.364907819453274,\n",
       "  'f1-score': 0.3745513866231648,\n",
       "  'support': 1573},\n",
       " '2': {'precision': 0.4808743169398907,\n",
       "  'recall': 0.5854956753160346,\n",
       "  'f1-score': 0.528052805280528,\n",
       "  'support': 1503},\n",
       " '3': {'precision': 0.6175438596491228,\n",
       "  'recall': 0.6675094816687737,\n",
       "  'f1-score': 0.6415552855407046,\n",
       "  'support': 1582},\n",
       " '4': {'precision': 0.6,\n",
       "  'recall': 0.6459074733096085,\n",
       "  'f1-score': 0.6221079691516709,\n",
       "  'support': 562},\n",
       " '5': {'precision': 0.6624087591240876,\n",
       "  'recall': 0.6927480916030534,\n",
       "  'f1-score': 0.6772388059701492,\n",
       "  'support': 524},\n",
       " '6': {'precision': 0.669195751138088,\n",
       "  'recall': 0.7875,\n",
       "  'f1-score': 0.723543888433142,\n",
       "  'support': 560},\n",
       " '7': {'precision': 0.5939479239971851,\n",
       "  'recall': 0.6482334869431644,\n",
       "  'f1-score': 0.6199045170767536,\n",
       "  'support': 1302},\n",
       " '8': {'precision': 0.7912621359223301,\n",
       "  'recall': 0.854521625163827,\n",
       "  'f1-score': 0.821676118462508,\n",
       "  'support': 763},\n",
       " '9': {'precision': 0.5150753768844221,\n",
       "  'recall': 0.37822878228782286,\n",
       "  'f1-score': 0.4361702127659574,\n",
       "  'support': 542},\n",
       " '10': {'precision': 0.5656324582338902,\n",
       "  'recall': 0.5715434083601286,\n",
       "  'f1-score': 0.5685725709716114,\n",
       "  'support': 1244},\n",
       " '11': {'precision': 0.5992578849721707,\n",
       "  'recall': 0.5218093699515347,\n",
       "  'f1-score': 0.5578583765112263,\n",
       "  'support': 1238},\n",
       " '12': {'precision': 0.7193396226415094,\n",
       "  'recall': 0.741190765492102,\n",
       "  'f1-score': 0.7301017354877319,\n",
       "  'support': 823},\n",
       " 'accuracy': 0.5767692307692308,\n",
       " 'macro avg': {'precision': 0.5894533039845059,\n",
       "  'recall': 0.5888268492902934,\n",
       "  'f1-score': 0.582770729934713,\n",
       "  'support': 13000},\n",
       " 'weighted avg': {'precision': 0.5708601547940884,\n",
       "  'recall': 0.5767692307692308,\n",
       "  'f1-score': 0.5686211324477896,\n",
       "  'support': 13000}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(df_train['category_id'][70000:], np.argmax(y_pred, axis=1), output_dict=True)\n",
    "\n",
    "# classification_report(df_train2['category_id'][90000:], np.argmax(y_pred2, axis=1), output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/kaggle/working/reddit_predictor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"/kaggle/working/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'dense_1/Softmax:0' shape=(None, 13) dtype=float32>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense_1/Softmax']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_names = [x.op.name for x in model.outputs]\n",
    "out_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "#     \"\"\"\n",
    "#     Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "#     Creates a new computation graph where variable nodes are replaced by\n",
    "#     constants taking their current value in the session. The new graph will be\n",
    "#     pruned so subgraphs that are not necessary to compute the requested\n",
    "#     outputs are removed.\n",
    "#     @param session The TensorFlow session to be frozen.\n",
    "#     @param keep_var_names A list of variable names that should not be frozen,\n",
    "#                           or None to freeze all the variables in the graph.\n",
    "#     @param output_names Names of the relevant graph outputs.\n",
    "#     @param clear_devices Remove the device directives from the graph for better portability.\n",
    "#     @return The frozen graph definition.\n",
    "#     \"\"\"\n",
    "#     graph = session.graph\n",
    "#     with graph.as_default():\n",
    "#         freeze_var_names = list(set(v.op.name for v in tf.compat.v1.global_variables()).difference(keep_var_names or []))\n",
    "#         output_names = output_names or []\n",
    "#         output_names += [v.op.name for v in tf.compat.v1.global_variables()]\n",
    "#         input_graph_def = graph.as_graph_def()\n",
    "#         if clear_devices:\n",
    "#             for node in input_graph_def.node:\n",
    "#                 node.device = \"\"\n",
    "#         frozen_graph = tf.compat.v1.graph_util.convert_variables_to_constants(\n",
    "#             session, input_graph_def, output_names, freeze_var_names)\n",
    "#         return frozen_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras import backend as K\n",
    "\n",
    "# # frozen_graph = freeze_session(tf.compat.v1.Session(),\n",
    "# #                               output_names=[out.op.name for out in model.outputs])\n",
    "\n",
    "# frozen_graph = freeze_session(tf.compat.v1.Session(),\n",
    "#                               output_names=['dense_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The export path contains the name and the version of the model\n",
    "tf.keras.backend.set_learning_phase(0) # Ignore dropout at inference\n",
    "model = tf.keras.models.load_model('/kaggle/working/reddit_predictor.h5')\n",
    "export_path = '/kaggle/working/model2.pb'\n",
    "\n",
    "# Fetch the Keras session and save the model\n",
    "# The signature definition is defined by the input and output tensors\n",
    "# And stored with the default serving key\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    tf.compat.v1.saved_model.simple_save(\n",
    "        sess,\n",
    "        export_path,\n",
    "        inputs={'input_image': model.input},\n",
    "        outputs={t.name:t for t in model.outputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_name = \"/kaggle/working/model2.pb/saved_model.pb\"\n",
    "\n",
    "file_stats = os.stat(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size in MegaBytes is 0.000232696533203125\n"
     ]
    }
   ],
   "source": [
    "print(f'File Size in MegaBytes is {file_stats.st_size / (1024 * 1024)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.io.write_graph(frozen_graph, \"/kaggle/working\", \"reddit_trained_tf.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_model= model.to_yaml()\n",
    "# writing the yaml model to the yaml file\n",
    "with open('/kaggle/working/yamlmodel.yaml', 'w') as yaml_file:\n",
    "    yaml_file.write(yaml_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.python.framework import graph_util\n",
    "# from tensorflow.python.framework import graph_io\n",
    "# from pathlib import Path\n",
    "# from absl import app\n",
    "# from absl import flags\n",
    "# from absl import logging\n",
    "# import keras\n",
    "# from keras import backend as K\n",
    "# from keras.models import model_from_json, model_from_yaml\n",
    "\n",
    "# K.set_learning_phase(0)\n",
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "# # def del_all_flags(FLAGS):\n",
    "# #     flags_dict = FLAGS._flags()\n",
    "# #     keys_list = [keys for keys in flags_dict]\n",
    "# #     for keys in keys_list:\n",
    "# #         FLAGS.delattr(keys)\n",
    "\n",
    "# # del_all_flags(FLAGS)\n",
    "\n",
    "# # def del_all_flags(FLAGS):\n",
    "# #     flags_dict = FLAGS._flags()\n",
    "# # #     keys_list = [keys for keys in flags_dict]\n",
    "# #     for keys, values in flags_dict.items():\n",
    "# #         delattr(keys, values)\n",
    "\n",
    "# # del_all_flags(FLAGS)\n",
    "\n",
    "# def del_all_flags(FLAGS):\n",
    "#     flags_dict = FLAGS._flags()\n",
    "#     keys_list = [keys for keys in flags_dict]\n",
    "#     for keys in keys_list:\n",
    "#         FLAGS.__delattr__(keys)\n",
    "        \n",
    "# del_all_flags(FLAGS)\n",
    "\n",
    "# flags.DEFINE_string('input_model2', None, '/kaggle/working/reddit_predictor.h5')\n",
    "# flags.DEFINE_string('input_model_json', None, '/kaggle/working/model.json')\n",
    "# flags.DEFINE_string('input_model_yaml', None, '/kaggle/working/yamlmodel.yaml')\n",
    "# flags.DEFINE_string('output_model', None, 'letsee.pb')\n",
    "# flags.DEFINE_boolean('save_graph_def', False,\n",
    "#                      'Whether to save the graphdef.pbtxt file which contains '\n",
    "#                      'the graph definition in ASCII format.')\n",
    "# # flags.DEFINE_string('output_nodes_prefix', None,\n",
    "# #                     'If set, the output nodes will be renamed to '\n",
    "# #                     '`output_nodes_prefix`+i, where `i` will numerate the '\n",
    "# #                     'number of of output nodes of the network.')\n",
    "# # flags.DEFINE_boolean('quantize', False,\n",
    "# #                      'If set, the resultant TensorFlow graph weights will be '\n",
    "# #                      'converted from float into eight-bit equivalents. See '\n",
    "# #                      'documentation here: '\n",
    "# #                      'https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms')\n",
    "# flags.DEFINE_boolean('channels_first', False,\n",
    "#                      'Whether channels are the first dimension of a tensor. '\n",
    "#                      'The default is TensorFlow behaviour where channels are '\n",
    "#                      'the last dimension.')\n",
    "# flags.DEFINE_boolean('output_meta_ckpt', False,\n",
    "#                      'If set to True, exports the model as .meta, .index, and '\n",
    "#                      '.data files, with a checkpoint file. These can be later '\n",
    "#                      'loaded in TensorFlow to continue training.')\n",
    "\n",
    "# flags.mark_flag_as_required('input_model2')\n",
    "# flags.mark_flag_as_required('output_model')\n",
    "\n",
    "\n",
    "# def load_model(input_model_path, input_json_path=None, input_yaml_path=None):\n",
    "#     if not Path(input_model_path).exists():\n",
    "#         raise FileNotFoundError(\n",
    "#             'Model file `{}` does not exist.'.format(input_model_path))\n",
    "#     try:\n",
    "#         model = keras.models.load_model(input_model_path)\n",
    "#         return model\n",
    "#     except FileNotFoundError as err:\n",
    "#         logging.error('Input mode file (%s) does not exist.', FLAGS.input_model2)\n",
    "#         raise err\n",
    "#     except ValueError as wrong_file_err:\n",
    "#         if input_json_path:\n",
    "#             if not Path(input_json_path).exists():\n",
    "#                 raise FileNotFoundError(\n",
    "#                     'Model description json file `{}` does not exist.'.format(\n",
    "#                         input_json_path))\n",
    "#             try:\n",
    "#                 model = model_from_json(open(str(input_json_path)).read())\n",
    "#                 model.load_weights(input_model_path)\n",
    "#                 return model\n",
    "#             except Exception as err:\n",
    "#                 logging.error(\"Couldn't load model from json.\")\n",
    "#                 raise err\n",
    "#         elif input_yaml_path:\n",
    "#             if not Path(input_yaml_path).exists():\n",
    "#                 raise FileNotFoundError(\n",
    "#                     'Model description yaml file `{}` does not exist.'.format(\n",
    "#                         input_yaml_path))\n",
    "#             try:\n",
    "#                 model = model_from_yaml(open(str(input_yaml_path)).read())\n",
    "#                 model.load_weights(input_model_path)\n",
    "#                 return model\n",
    "#             except Exception as err:\n",
    "#                 logging.error(\"Couldn't load model from yaml.\")\n",
    "#                 raise err\n",
    "#         else:\n",
    "#             logging.error(\n",
    "#                 'Input file specified only holds the weights, and not '\n",
    "#                 'the model definition. Save the model using '\n",
    "#                 'model.save(filename.h5) which will contain the network '\n",
    "#                 'architecture as well as its weights. '\n",
    "#                 'If the model is saved using the '\n",
    "#                 'model.save_weights(filename) function, either '\n",
    "#                 'input_model_json or input_model_yaml flags should be set to '\n",
    "#                 'to import the network architecture prior to loading the '\n",
    "#                 'weights. \\n'\n",
    "#                 'Check the keras documentation for more details '\n",
    "#                 '(https://keras.io/getting-started/faq/)')\n",
    "#             raise wrong_file_err\n",
    "\n",
    "\n",
    "# def main(args):\n",
    "#     # If output_model path is relative and in cwd, make it absolute from root\n",
    "#     output_model = FLAGS.output_model\n",
    "#     if str(Path(output_model).parent) == '.':\n",
    "#         output_model = str((Path.cwd() / output_model))\n",
    "\n",
    "#     output_fld = Path(output_model).parent\n",
    "#     output_model_name = Path(output_model).name\n",
    "#     output_model_stem = Path(output_model).stem\n",
    "#     output_model_pbtxt_name = output_model_stem + '.pbtxt'\n",
    "\n",
    "#     # Create output directory if it does not exist\n",
    "#     Path(output_model).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     if FLAGS.channels_first:\n",
    "#         K.set_image_data_format('channels_first')\n",
    "#     else:\n",
    "#         K.set_image_data_format('channels_last')\n",
    "\n",
    "#     model = load_model(FLAGS.input_model2, FLAGS.input_model_json, FLAGS.input_model_yaml)\n",
    "\n",
    "#     # TODO(amirabdi): Support networks with multiple inputs\n",
    "#     orig_output_node_names = [node.op.name for node in model.outputs]\n",
    "#     if FLAGS.output_nodes_prefix:\n",
    "#         num_output = len(orig_output_node_names)\n",
    "#         pred = [None] * num_output\n",
    "#         converted_output_node_names = [None] * num_output\n",
    "\n",
    "#         # Create dummy tf nodes to rename output\n",
    "#         for i in range(num_output):\n",
    "#             converted_output_node_names[i] = '{}{}'.format(\n",
    "#                 FLAGS.output_nodes_prefix, i)\n",
    "#             pred[i] = tf.identity(model.outputs[i],\n",
    "#                                   name=converted_output_node_names[i])\n",
    "#     else:\n",
    "#         converted_output_node_names = orig_output_node_names\n",
    "#     logging.info('Converted output node names are: %s',\n",
    "#                  str(converted_output_node_names))\n",
    "\n",
    "#     sess = K.get_session()\n",
    "#     if FLAGS.output_meta_ckpt:\n",
    "#         saver = tf.train.Saver()\n",
    "#         saver.save(sess, str(output_fld / output_model_stem))\n",
    "\n",
    "#     if FLAGS.save_graph_def:\n",
    "#         tf.train.write_graph(sess.graph.as_graph_def(), str(output_fld),\n",
    "#                              output_model_pbtxt_name, as_text=True)\n",
    "#         logging.info('Saved the graph definition in ascii format at %s',\n",
    "#                      str(Path(output_fld) / output_model_pbtxt_name))\n",
    "\n",
    "#     if FLAGS.quantize:\n",
    "#         from tensorflow.tools.graph_transforms import TransformGraph\n",
    "#         transforms = [\"quantize_weights\", \"quantize_nodes\"]\n",
    "#         transformed_graph_def = TransformGraph(sess.graph.as_graph_def(), [],\n",
    "#                                                converted_output_node_names,\n",
    "#                                                transforms)\n",
    "#         constant_graph = graph_util.convert_variables_to_constants(\n",
    "#             sess,\n",
    "#             transformed_graph_def,\n",
    "#             converted_output_node_names)\n",
    "#     else:\n",
    "#         constant_graph = graph_util.convert_variables_to_constants(\n",
    "#             sess,\n",
    "#             sess.graph.as_graph_def(),\n",
    "#             converted_output_node_names)\n",
    "\n",
    "#     graph_io.write_graph(constant_graph, str(output_fld), output_model_name,\n",
    "#                          as_text=False)\n",
    "#     logging.info('Saved the freezed graph at %s',\n",
    "#                  str(Path(output_fld) / output_model_name))\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
